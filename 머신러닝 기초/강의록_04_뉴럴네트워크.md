## 뉴럴네트워크(신경망) 모델
1. [신경망 개요](#1장-신경망-개요)   
2. [파라미터 추정](#2장-파라미터-추정)   
3. [실습](#실습)   

### 1장 신경망 개요
- 신경망 : 단층 퍼셉트론
  - 단층 퍼셉트론 - 초기 뉴럴네트워크
  - 입력값의 선형결합 값을 구하고 그 값이 0보다 큰지 여부를 분류
  - ![캡처](https://user-images.githubusercontent.com/43491168/111023441-6c8c0b00-841c-11eb-9363-5d4615ac867a.PNG)
  - 단층 퍼셉트론의 한계 : XOR문제 해결 불가
    - OR[(0,0)=0, (0,1)=1, (1,0)=1, (1,1)=1], AND[(0,0)=0, (0,1)=0, (1,0)=0, (1,1)=1], XOR[(0,0)=0, (0,1)=1, (1,0)=1, (1,1)=0]
    - ![캡처](https://user-images.githubusercontent.com/43491168/111023541-0358c780-841d-11eb-8860-0f079f9c19a0.PNG)

- 신경망 : 2중 퍼셉트론
  - 2개 이상의 퍼셉트론 = 다층 퍼셉트론, 2중 퍼셉트론 부터 뉴럴네트워크
  - ![캡처](https://user-images.githubusercontent.com/43491168/111023633-72ceb700-841d-11eb-9ac6-70f120f31620.PNG)

- 신경망 : 다층 퍼셉트론
  - ![캡처](https://user-images.githubusercontent.com/43491168/111023669-af021780-841d-11eb-9b9d-ecb3f99a3fa2.PNG)
  - 출력층
    - 범주형 : 출력노드의 수 = 출력변수의 범주 개수
    - 연속형 : 출력노드의 수 = 출력변수의 개수

- 파라미터
  - 파라미터 : 층 간 노드를 연결하는 가중치, 알고리즘으로 결정됨
  - 하이퍼파라미터 : 은닉측 개수, 은닉노드 개수, Activation Function 등 사용자가 임의로 결정

- Activation Function(활성함수)
  - 활성함수 예시
    - ![캡처](https://user-images.githubusercontent.com/43491168/111023805-64cd6600-841e-11eb-9c20-2cab6e950cdb.PNG)

- 뉴럴네트워크 파라미터 결정
  - 비용함수(Cost function) : n-net 모델로부터 나온 Y값과 실제 Y값의 차이를 최소로 하는 가중치
    - 수치예측 : MSE 등
    - 분류 : Cross Entropy 등

- 경사하강법(Gradient descent method)
  - Gradient : 함수의 기울기
  - Turning Point의 개수는 함수의 차수에 의해 결정
  - 모든 Turning Point가 최솟값 또는 최댓값은 아님
  - ![캡처](https://user-images.githubusercontent.com/43491168/111023986-8d099480-841f-11eb-91f0-196d5d06d46e.PNG)
  - w(t+1) - a*L'(w(t)), 0 < a < 1 : leaning rate
  - ![캡처](https://user-images.githubusercontent.com/43491168/111024039-d659e400-841f-11eb-8685-443fb4f2b30c.PNG)

### 2장 파라미터 추정
- 뉴럴네트워크 학습
  - ![캡처](https://user-images.githubusercontent.com/43491168/111085131-56379980-8559-11eb-9731-ec7c21cbe0aa.PNG)

- 뉴럴네트워크 학습 알고리즘(과정)
  - Step1 : 모든 가중치(w)를 임의 생성
  - Step2-1 : 입력변수 값과 입력층과 은닉층 사이의 w값을 이용하여 은닉노드의 값을 계산 -> 선형결합 후 activation
  - Step2-2 : 은닉노드 값과 은닉층과 출력층 사이의 w값을 이용하여 출력노드의 값을 계산 -> 선형결합 후 activation
  - Step3-1 : 계산된 출력노드의 값과 실제 출력변수의 값의 차이를 줄일 수 있도록 가중치(w) 업데이트(은닉층, 출력층 사이)
  - Step3-2 : 계산된 출력노드의 값과 실제 출력변수의 값의 차이를 줄일 수 있도록 가중치(w) 업데이트(입력층, 은닉층 사이)
  - 에러가 충분히 줄어들때 까지 Step2 ~ 3을 반복 = 오류 역전파 알고리즘(Error Backpropagation Algorithm)

### 실습
