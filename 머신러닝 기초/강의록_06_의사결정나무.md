  ## 의사결정나무
1. [의사결정나무 개요](#1장-의사결정나무-모델)   
2. [예측나무 모델](#2장-예측나무-모델)   
3. [분류나무 모델](#3장-분류나무-모델)   
4. [Information Gain](#4장-Information-Gain)   

### 1장 의사결정나무 개요
- 데이터에 내재되어 있는 패턴을 변수의 조합으로 나무의 형태로 만드는 것
  - ![캡처](https://user-images.githubusercontent.com/43491168/111871934-04b36280-89d0-11eb-9591-e928f44c8e11.PNG)

### 2장 예측나무 모델
- Regression Model
  - ![캡처](https://user-images.githubusercontent.com/43491168/111872066-ca969080-89d0-11eb-8f2e-33ce99805172.PNG)
  - 함수식 표현(I : Indicator function)
    - ![캡처](https://user-images.githubusercontent.com/43491168/111872651-37aa2600-89d1-11eb-8f3f-29ac4dc8c59f.PNG)
- 예측나무 모델링 프로세스
  - ![캡처](https://user-images.githubusercontent.com/43491168/111873018-a12a3480-89d1-11eb-8e05-808a3bfb26eb.PNG)
  - 분할변수(j), 분할점(s) 결정 기준 
    - (y - c)를 최소화 하는 c=해당 부분집합 내 평균값.
    - ![캡처](https://user-images.githubusercontent.com/43491168/111873251-c53a4580-89d2-11eb-8a4b-59d284f3b55d.PNG)

### 3장 분류나무 모델
- Classification Model
  - ![캡처](https://user-images.githubusercontent.com/43491168/111873387-70e39580-89d3-11eb-9922-bfe1a53676b3.PNG)
  - 함수식 표현
    - k : 클래스(범주), p_hat : 끝노드 m에서 k클래스에 대한 관측치 비율 
    - ![캡처](https://user-images.githubusercontent.com/43491168/111873483-c5871080-89d3-11eb-9b1c-a03d48e0dd64.PNG)
- 분류나무 모델링 프로세스
  - 비용함수의 종류
    - ![캡처](https://user-images.githubusercontent.com/43491168/111873610-81484000-89d4-11eb-9163-110569746963.PNG)
    - Cross-entropy는 scale된 결과임
  - 분할변수(j), 분할점(s) 결정 기준 
    - ![캡처](https://user-images.githubusercontent.com/43491168/111873710-ffa4e200-89d4-11eb-8646-f55727ded577.PNG)
    - 불할법칙
      - 분할변수, 분할기준은 y의 분포를 가장 잘 구별해주는 쪽으로 정함.
      - y의 분포를 잘 구별해주는 측도로 순수도(purity) 또는 불순도(impurity)를 정의
      - 각 노드에서 분할변수와 분할점의 설정은 불순도의 감소가 최대가 되도록 선택

### 4장 Information Gain
![캡처](https://user-images.githubusercontent.com/43491168/111873955-316a7880-89d6-11eb-8f8b-17b0c1941580.PNG)

### 5장 개별 트리 모델의 단점
- 계층적 구조로 인해 중간에 에러가 발생하면 다음 단계로 에러가 계속 전파
- 학습데이터의 미세한 변동에도 최종 결과에 크게 영향, 적은 개수의 노이즈에도 크게 영향
- leaf노드의 개수를 늘리면 과적함 위험 
- ==> 해결방안 : Random forest
