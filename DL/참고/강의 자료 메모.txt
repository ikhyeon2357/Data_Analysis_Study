# ML lab 07 : 개념
* gradient descent
    1. Data(x) preprocessing for gradient descent
        - x값 간의 단위 차이로인해 gradient descent중 값이 튈 수 있음.(단위 차이로 인해 learning rate에 대한 이동 단위가 다른듯)
        - 방법 : zero-centered data(중심을 0으로 이동), normalized data((x-평균)/분산)

* Overfittint
    1. Solution for overfitting
        - More training data
        - Reduce the number of features
        - Reqularization : reqularization strength(람다*시그마(W), 기존 L1, L2 norm과 유사)


* online / offline learning
    1. online learning <--> offline learning
        - online learning : 새로운 데이터를 실시간으로 학습, 신규 데이터가 들어오면 기존 모델에 추가? 학습
        - offline learning : 학습 데이터를 한번에 학습(batch learning), 신규 데이터가 들어오면 처음부터 다시 학습 
        - 참고 : https://choihomeboy.tistory.com/3
        - 참고 : https://m.blog.naver.com/pertisia/222038150105

* Training epoch / batch / Iteration 개념
    1. epoch : 전체 데이터가 한번 학습하는 과정 = 1 epoch
    2. batch : 한번의 배치마다 주는 데이터 샘플의 수
    3. iteration : epoch을 나누어 실행하는 횟수( = 1epoch을 마치는데 필요한 파라미터 업데이트 횟수)
        - 1000개 데이터를 100개 batch로 10번 iteration으로 15회(epoch) 학습


# ML lab 08 : 개념
* xor 문제
    - 단일 선형으로 구분 불가
    1. 해결 : multilayer neural nets
        - 문제 : multilayer에서 weight업데이트 불가
    2. 해결 : backpropagation
        - Convolutional Neural Networks(왜 여기에 같이 나오는지 모르겠음...)
        - 문제 : layer가 길어지면 backpropagation으로 w업데이트시 (앞으로?)전달이 잘 안됨(복잡해 질수록 성능 저하)

# ML lab 09 : XOR 문제
* Multilayer neural nets
    - 참고 : https://ang-love-chang.tistory.com/26?category=878662
    
    1. 식(예시), 참고/ 이미지 파일 참고
        -  k(x) = sigmoid(X*W1 + B1)
        -  H(x) = sigmoid(k(x)*W2 + B2)

# ML lab 10 : Deep & Wide
    1. Sigmoid 보다 ReLU(Rectified Linear Unit)가 좋은 이유
        - Sigmoid의 경우 chain rule에 의해 뒤로 갈수록(0~1 사이에 값을 계속 곱하면서 발생) 미분값이 0에 가까워짐(Vanishing gradient)
        - L1 = tf.nn.relu(tf.matmul(X, W1) + b1)
        - 참고 : https://pythonkim.tistory.com/40?category=573319
        
    2. (W)초기값 문제
        - Need to set the initial weight values wisely
            ' Not all 0's(Weight 업데이트 안됨)
            ' Restricted Boatman Machine(RBM), Deep Belief Nets : https://pythonkim.tistory.com/41?category=573319
                 : 위 방법으로 W를 초기화 해서 학스할 경우 학습 시간이 얼마 걸리지 않는다.
                 그래서 학습(learning)이 아니라 Fine Tuning이라고 부름.
        - xavier, He 등 대안
            ' RBM의 복잡성을 간단한 방법으로 성능은 낼 수 있음.
            ' xavier - 입력값과 출력값 사이의 난수를 선택해서 입력값의 제곱근으로 나눈다.
            ' he - 입력값을 반으로 나눈 제곱근 사용. 분모가 작아지기 때문에 xavier보다 넓은 범위의 난수 생성
      
    3. Dropout과 앙상블(Overfitting)
        - 과적합 대안
            ' 학습 데이터 추가, 변수의 갯수 축소, 정규화
        - Dropout : 신경망의 일부를 비활성화(학습중에만 n% dropout, 예측시 100% 사용)
        - Ensemble...은 앙상블

# ML lab 11 : CNN(Convolution Neural Network)
    1. CNN 기초
        - 참고 : 
        - Start with an image(width x hight x depth(RGB) - color image)
            ' Exmaple : 32x32x3 -> 5x5x3(이미지의 일부만 사용) -> 이미지를 이동하면서 확인(stride : 이동 간격), 이때 동일한 Weight 사용
            ' Output size : (N - F) / stride + 1(N : 가로 길이(32), F : 부분의 가로 길이(5)) 
            ' 단점 : 이미지가 점점 작아짐(정보 손실)
        - Padding : 입력 이미지의 크기와 출력 이미지의 크기가 동일하도록 입력 이미지의 태두리에 '0'값을 추가
            ' Example : 7x7x3(stride = 1일때 정보 손실 방지) -> pad=1 -> 9x9x3 이미지
        - Weight의 개수
            ' Conv Layer shape x filter 수 : ex) 5x5x3(shape)x6(filter 수)
    2. CNN - Pooling, Full Network
        - Pooling(= Sampling)
            ' conv layer에서 작은 사이즈로 Sampling
            ' Example : 4x4(Single depth slice) -> Max pool, 2x2 filter, stride=2 -> 2x2 이미지(각 filter에 Max값 사용)
        - Fullty Connected Layer (FC layer)
            ' 마지막에 일반적인 n-net을 통해 구성(ex : Softmax)
     3. CNN - ConvNet 활용
         - AlexNet
             ' 227x227x3 Images -> Conv_1 = 11x11x3, stride = 4 -> Output : 55x55x3x96 -> Parameters : (11*11*3)*96 = 약 35k -> ...... -> FC
         - GoogLeNet
             ' Inception module
         - ResNet
             ' 레이어를 점프해서 학습(Layer가 깊지만 합쳐?지는 효과로 학습)
         - DeepMind's