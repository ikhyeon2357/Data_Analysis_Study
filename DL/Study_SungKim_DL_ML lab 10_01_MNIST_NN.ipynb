{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6068e84a-e720-426d-bccf-fd43d744889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 요약\n",
    "#### Softmax----------------: 0.8946\n",
    "#### NN---------------------: 0.9271\n",
    "#### NN_xavier--------------: 0.9706\n",
    "#### NN_xavier_Deep---------: 0.9653(과적합으로 정확도 감소)\n",
    "#### NN_xavier_Deep_Dropout-: 0.9735\n",
    "\n",
    "## Optimizer : 보통 Adam 많이쓰고 권장함(지금도 그런가?)\n",
    "## Batch Normalization(일단 pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "098f9f08-a7c3-49dd-840d-43859e84847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf_load_data\n",
    "from keras.utils import np_utils\n",
    "(x_train, y_train), (x_test, y_test) = tf_load_data.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a25c26-a037-4e47-bdca-58615e7280f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f589d7-0098-4884-ac5e-97419c7043ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제용 reshape\n",
    "x_train, x_test = x_train/255.0, x_test/255.0   #Feature scaling 적용, 그대록 학습시 문제됨...\n",
    "\n",
    "# [[], [], ...] --> [...]\n",
    "x_train_rs = x_train.reshape(x_train.shape[0], 784).astype('float32')\n",
    "x_test_rs = x_test.reshape(x_test.shape[0], 784).astype('float32')\n",
    "\n",
    "# one hot encoding\n",
    "y_train_onehot = np_utils.to_categorical(y_train)\n",
    "y_test_onehot = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beca7718-5f0f-4f78-bf8b-d1698d2d931c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 10)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_rs.shape)\n",
    "print(y_train_onehot.shape)\n",
    "print(y_test_onehot.shape)\n",
    "print(x_test_rs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3356b901-1ec2-490d-8b1c-8c26c516c97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kih\\Anaconda3\\envs\\tf_py36\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0612d005-ff9d-4618-a621-863938fd5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d24f795-58a4-4f08-810f-f49b3cdcc580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37195d8d-6169-46b6-adda-54fc20af8e31",
   "metadata": {},
   "source": [
    "# SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d94eecc-cdda-425e-8513-52a55b04b1ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7cb567c-519d-4a1b-b2cd-4123f3b5f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "num_epochs = 15\n",
    "batch_size = 100\n",
    "#num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "num_iterations = int(x_train.shape[0] / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79266476-900e-485b-879b-5a21fee5ea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Cost: 2.833290844\n",
      "Epoch: 0002, Cost: 1.097388055\n",
      "Epoch: 0003, Cost: 0.863086824\n",
      "Epoch: 0004, Cost: 0.739014846\n",
      "Epoch: 0005, Cost: 0.673302667\n",
      "Epoch: 0006, Cost: 0.622510433\n",
      "Epoch: 0007, Cost: 0.587486926\n",
      "Epoch: 0008, Cost: 0.554646429\n",
      "Epoch: 0009, Cost: 0.529113251\n",
      "Epoch: 0010, Cost: 0.518516070\n",
      "Epoch: 0011, Cost: 0.480312823\n",
      "Epoch: 0012, Cost: 0.473730461\n",
      "Epoch: 0013, Cost: 0.475712282\n",
      "Epoch: 0014, Cost: 0.463228252\n",
      "Epoch: 0015, Cost: 0.444416175\n",
      "Learning finished\n",
      "Accuracy:  0.8946\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            #batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            #_, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            batch_xs, batch_ys = next_batch(batch_size, x_train_rs, y_train_onehot)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            #print(cost_val)\n",
    "            #print(num_iterations)\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "    \n",
    "    # Test the model using test sets\n",
    "    print(\n",
    "        \"Accuracy: \",\n",
    "        accuracy.eval(\n",
    "            #session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
    "            session=sess, feed_dict={X: x_test_rs, Y: y_test_onehot}\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f3556-5675-44f3-9278-4d9c4134d863",
   "metadata": {},
   "source": [
    "# NN\n",
    "### - Deep, relu 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3d204e9-1411-489a-9277-2367e7247fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kih\\Anaconda3\\envs\\tf_py36\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ffd42-952e-46ca-a05d-5cb3d52786b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning finished\")\n",
    "    \n",
    "    # Test the model using test sets\n",
    "    print(\n",
    "        \"Accuracy: \",\n",
    "        accuracy.eval(\n",
    "            #session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
    "            session=sess, feed_dict={X: x_test_rs, Y: y_test_onehot}\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf2f3c9f-5b25-4ed1-9147-fbef5e5eba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 23.240231164\n",
      "Epoch: 0002 cost = 23.599039111\n",
      "Epoch: 0003 cost = 20.681526769\n",
      "Epoch: 0004 cost = 19.167079546\n",
      "Epoch: 0005 cost = 19.179179697\n",
      "Epoch: 0006 cost = 15.772219950\n",
      "Epoch: 0007 cost = 16.080380872\n",
      "Epoch: 0008 cost = 15.125584289\n",
      "Epoch: 0009 cost = 15.648667257\n",
      "Epoch: 0010 cost = 15.000378034\n",
      "Epoch: 0011 cost = 14.346532525\n",
      "Epoch: 0012 cost = 11.912314988\n",
      "Epoch: 0013 cost = 12.110262533\n",
      "Epoch: 0014 cost = 12.097198464\n",
      "Epoch: 0015 cost = 10.934589062\n",
      "Learning Finished!\n",
      "Accuracy: 0.9271\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    #total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    total_batch = 100\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        #batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs, batch_ys = next_batch(batch_size, x_train_rs, y_train_onehot)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#print('Accuracy:', sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={X: x_test_rs, Y: y_test_onehot}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b384d-1f03-4cfe-9a8a-a775b85ef978",
   "metadata": {},
   "source": [
    "# NN_xavier\n",
    "### - Deep, relu 사용 + xavier방법으로 Weight 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84a5d5-9f16-49b4-9343-99839331b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 버전에서는 \"initializer=tf.contrib.layers.xavier_initializer())\" 코드 사용불가(contrib 없는듯...)\n",
    "# 아래 링크 참고로 유사한 기능에 코드로 대체\n",
    "# https://stackoverflow.com/questions/59644859/attributeerror-module-tensorflow-core-compat-v1-has-no-attribute-contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "478a805f-791c-4b04-bae7-904b6938e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
    "                     #initializer=tf.contrib.layers.xavier_initializer())\n",
    "                     initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256],\n",
    "                     #initializer=tf.contrib.layers.xavier_initializer())\n",
    "                     initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10],\n",
    "                     #initializer=tf.contrib.layers.xavier_initializer())\n",
    "                     initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40b39f58-d039-4608-8bda-e3f33e3395ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.638998076\n",
      "Epoch: 0002 cost = 0.260522997\n",
      "Epoch: 0003 cost = 0.203825704\n",
      "Epoch: 0004 cost = 0.175977691\n",
      "Epoch: 0005 cost = 0.130951064\n",
      "Epoch: 0006 cost = 0.127342791\n",
      "Epoch: 0007 cost = 0.109459504\n",
      "Epoch: 0008 cost = 0.091833481\n",
      "Epoch: 0009 cost = 0.092433945\n",
      "Epoch: 0010 cost = 0.087961134\n",
      "Epoch: 0011 cost = 0.079305443\n",
      "Epoch: 0012 cost = 0.077013325\n",
      "Epoch: 0013 cost = 0.068665789\n",
      "Epoch: 0014 cost = 0.068031969\n",
      "Epoch: 0015 cost = 0.056188521\n",
      "Learning Finished!\n",
      "Accuracy: 0.9706\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    #total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    total_batch = 100\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        #batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs, batch_ys = next_batch(batch_size, x_train_rs, y_train_onehot)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#print('Accuracy:', sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={X: x_test_rs, Y: y_test_onehot}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7329a1-38ab-4539-9246-c346fea6d393",
   "metadata": {},
   "source": [
    "# NN_xavier_Deep\n",
    "### - Deep, relu 사용 + xavier방법으로 Weight 초기화\n",
    "### + Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ffd241d-4d23-470d-bdb1-9ed35b3791f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "546ab1ac-32fe-48b7-9cab-ddf9944d899d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kih\\Anaconda3\\envs\\tf_py36\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     #initializer=tf.contrib.layers.xavier_initializer())\n",
    "                     initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     #initializer=tf.contrib.layers.xavier_initializer())\n",
    "                     initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     #initializer=tf.contrib.layers.xavier_initializer())\n",
    "                     initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     #initializer=tf.contrib.layers.xavier_initializer())\n",
    "                     initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     #initializer=tf.contrib.layers.xavier_initializer())\n",
    "                     initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d89be279-230b-4c17-a14a-5b70409447aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 1.280968966\n",
      "Epoch: 0002 cost = 0.232107718\n",
      "Epoch: 0003 cost = 0.180386321\n",
      "Epoch: 0004 cost = 0.172244429\n",
      "Epoch: 0005 cost = 0.127462742\n",
      "Epoch: 0006 cost = 0.115443002\n",
      "Epoch: 0007 cost = 0.099368928\n",
      "Epoch: 0008 cost = 0.097800222\n",
      "Epoch: 0009 cost = 0.081836102\n",
      "Epoch: 0010 cost = 0.074979750\n",
      "Epoch: 0011 cost = 0.077808304\n",
      "Epoch: 0012 cost = 0.081205846\n",
      "Epoch: 0013 cost = 0.067910093\n",
      "Epoch: 0014 cost = 0.081668268\n",
      "Epoch: 0015 cost = 0.065532887\n",
      "Learning Finished!\n",
      "Accuracy: 0.9653\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    #total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    total_batch = 100\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        #batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs, batch_ys = next_batch(batch_size, x_train_rs, y_train_onehot)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#print('Accuracy:', sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={X: x_test_rs, Y: y_test_onehot}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0373c6bc-6d0f-42c0-ae4c-1d1dd5af8c42",
   "metadata": {},
   "source": [
    "# NN_xavier_Deep_Dropout\n",
    "### - Deep, relu 사용 + xavier방법으로 Weight 초기화\n",
    "### + Deep + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "071b74f2-7c60-44de-9fbb-5cbfbbde7e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kih\\Anaconda3\\envs\\tf_py36\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\kih\\Anaconda3\\envs\\tf_py36\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "#total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     #initializer=tf.contrib.layers.xavier_initializer())\n",
    "                     initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     #initializer=tf.contrib.layers.xavier_initializer())\n",
    "                     initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     #initializer=tf.contrib.layers.xavier_initializer())\n",
    "                     initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     #initializer=tf.contrib.layers.xavier_initializer())\n",
    "                     initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     #initializer=tf.contrib.layers.xavier_initializer())\n",
    "                     initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a127375-8d00-4775-bcc9-8e760c578cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.133428596\n",
      "Epoch: 0002 cost = 0.111330585\n",
      "Epoch: 0003 cost = 0.106143504\n",
      "Epoch: 0004 cost = 0.108231284\n",
      "Epoch: 0005 cost = 0.099682710\n",
      "Epoch: 0006 cost = 0.094382100\n",
      "Epoch: 0007 cost = 0.099571837\n",
      "Epoch: 0008 cost = 0.082634827\n",
      "Epoch: 0009 cost = 0.096847114\n",
      "Epoch: 0010 cost = 0.093050286\n",
      "Epoch: 0011 cost = 0.090213534\n",
      "Epoch: 0012 cost = 0.089472233\n",
      "Epoch: 0013 cost = 0.086094005\n",
      "Epoch: 0014 cost = 0.081965483\n",
      "Epoch: 0015 cost = 0.081699012\n",
      "Learning Finished!\n",
      "Accuracy: 0.9735\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    #total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    total_batch = 100\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        #batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs, batch_ys = next_batch(batch_size, x_train_rs, y_train_onehot)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.8}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#print('Accuracy:', sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={X: x_test_rs, Y: y_test_onehot, keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e27ae9-a85e-4214-be39-4454ccec775a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83022709-5bb3-4fa9-a42e-93db446c63ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
