{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfW8WT8oeD6p"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minsuk-heo/tf2/blob/master/jupyter_notebooks/10.Word2Vec_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5I05bp7eD6s"
      },
      "source": [
        "# Sentence Classification using LSTM and Pretrained Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZG7QxxJeD6s"
      },
      "source": [
        "pretrain된 word2vec 모델과 LSTM을 활용환 텍스트 분류(긍/부정)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6GysnaEeD6s",
        "outputId": "3e03c585-e86d-475c-f1da-d51536d51113"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/minsuk-heo/tf2/master/img/10_train.png\" width=\"1024\" height=\"768\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/tf2/master/img/10_train.png\", width=800, height=300)\n",
        "\n",
        "#from IPython.display import Image\n",
        "#Image(url= \"https://raw.githubusercontent.com/minsuk-heo/tf2/master/img/10_test.png\", width=800, height=300)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrain 장점 : Train data에 없는 정보에 대해서도 예측가능(이미 더 큰 데이터로 학습됨.)"
      ],
      "metadata": {
        "id": "8InUEnWniHr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to understand the detail of Word2Vec, here is well explained video.\n",
        "# from IPython.display import YouTubeVideo\n",
        "# YouTubeVideo('64qSgA66P-8')\n",
        "# YouTubeVideo('m2cg32uEeZw')"
      ],
      "metadata": {
        "id": "cWtdxbSEheOL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jXnGB_iVeD6w"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2_X1GX_feD6x"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eqfusPOceD6x"
      },
      "outputs": [],
      "source": [
        "# Load Pretrained Word2Vec\n",
        "embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250/2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Jh58ro6IeD6x"
      },
      "outputs": [],
      "source": [
        "def get_max_length(df):\n",
        "    \"\"\"\n",
        "    - 최대 단어 수\n",
        "    get max token counts from train data, \n",
        "    so we use this number as fixed length input to RNN cell\n",
        "    \"\"\"\n",
        "    max_length = 0\n",
        "    for row in df['review']:\n",
        "        if len(row.split(\" \")) > max_length:\n",
        "            max_length = len(row.split(\" \"))\n",
        "    return max_length\n",
        "\n",
        "def get_word2vec_enc(reviews):\n",
        "    \"\"\"\n",
        "    - word2vec 변환\n",
        "    get word2vec value for each word in sentence.\n",
        "    concatenate word in numpy array, so we can use it as RNN input\n",
        "    \"\"\"\n",
        "    encoded_reviews = []\n",
        "    for review in reviews:\n",
        "        tokens = review.split(\" \")\n",
        "        word2vec_embedding = embed(tokens)\n",
        "        encoded_reviews.append(word2vec_embedding)\n",
        "    return encoded_reviews\n",
        "        \n",
        "def get_padded_encoded_reviews(encoded_reviews):\n",
        "    \"\"\"\n",
        "    - zero padding\n",
        "    for short sentences, we prepend zero padding so all input to RNN has same length\n",
        "    \"\"\"\n",
        "    padded_reviews_encoding = []\n",
        "    for enc_review in encoded_reviews:\n",
        "        zero_padding_cnt = max_length - enc_review.shape[0]\n",
        "        pad = np.zeros((1, 250))\n",
        "        for i in range(zero_padding_cnt):\n",
        "            enc_review = np.concatenate((pad, enc_review), axis=0)\n",
        "        padded_reviews_encoding.append(enc_review)\n",
        "    return padded_reviews_encoding\n",
        "\n",
        "def sentiment_encode(sentiment):\n",
        "    \"\"\"\n",
        "    - 긍/부정 endoding\n",
        "    return one hot encoding for Y value\n",
        "    \"\"\"\n",
        "    if sentiment == 'positive':\n",
        "        return [1,0]\n",
        "    else:\n",
        "        return [0,1]\n",
        "    \n",
        "def preprocess(df):\n",
        "    \"\"\"\n",
        "    encode text value to numeric value\n",
        "    \"\"\"\n",
        "    # encode words into word2vec\n",
        "    reviews = df['review'].tolist()\n",
        "    \n",
        "    encoded_reviews = get_word2vec_enc(reviews)\n",
        "    padded_encoded_reviews = get_padded_encoded_reviews(encoded_reviews)\n",
        "    # encoded sentiment\n",
        "    sentiments = df['sentiment'].tolist()\n",
        "    encoded_sentiment = [sentiment_encode(sentiment) for sentiment in sentiments]\n",
        "    X = np.array(padded_encoded_reviews)\n",
        "    Y = np.array(encoded_sentiment)\n",
        "    return X, Y "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JucAONr7eD6y"
      },
      "source": [
        "# Preprocess (encode text to number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q-F4J7qPeD6y"
      },
      "outputs": [],
      "source": [
        "movie_reviews_train = [\n",
        "         {'review': 'this is the best movie', 'sentiment': 'positive'},\n",
        "         {'review': 'i recommend you watch this movie', 'sentiment': 'positive'},\n",
        "         {'review': 'it was waste of money and time', 'sentiment': 'negative'},\n",
        "         {'review': 'the worst movie ever', 'sentiment': 'negative'}\n",
        "    ]\n",
        "df = pd.DataFrame(movie_reviews_train)\n",
        "\n",
        "# LSTM의 INPUT값이 고정값이기 때문에 MAX LENGTH 기준으로 일치 필요\n",
        "max_length = get_max_length(df)\n",
        "\n",
        "train_X, train_Y = preprocess(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhvv6wLSeD6z"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mRktt1MReD6z"
      },
      "outputs": [],
      "source": [
        "# LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(2, activation='softmax')) # 긍/부정 분류"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WKsopeF6eD6z"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX2VuAWfeD6z"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kp6v34a1eD6z",
        "outputId": "31d1389e-3b3a-44d8-f9ce-44c455d960ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train...\n",
            "Epoch 1/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6994 - accuracy: 0.5000\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6890 - accuracy: 0.5000\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6787 - accuracy: 1.0000\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6687 - accuracy: 1.0000\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6587 - accuracy: 1.0000\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6487 - accuracy: 1.0000\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6387 - accuracy: 1.0000\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6284 - accuracy: 1.0000\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6180 - accuracy: 1.0000\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6072 - accuracy: 1.0000\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5962 - accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5848 - accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5729 - accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5607 - accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5480 - accuracy: 1.0000\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5348 - accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5211 - accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5069 - accuracy: 1.0000\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4922 - accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4770 - accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4612 - accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4449 - accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4282 - accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4109 - accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3933 - accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.3753 - accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3570 - accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3385 - accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3197 - accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3009 - accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2820 - accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2632 - accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2444 - accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2259 - accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2076 - accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1898 - accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1724 - accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1556 - accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1395 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1242 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1098 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0965 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0843 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0732 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0633 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0546 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0469 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0403 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0347 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0299 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd167f9be90>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "print('Train...')\n",
        "model.fit(train_X, train_Y,epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GIFLWnYxeD60",
        "outputId": "c0bf8af2-4d93-461e-839a-f4841fdfb922",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 32)                36224     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 36,290\n",
            "Trainable params: 36,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 데이터의 nice, better 단어가 train에 없지만 best와 유사한 벡터값을 가지고 있으므로 예측가능(word2vec이 pretrain된 데이터 이므로.)"
      ],
      "metadata": {
        "id": "j-TNiP8PmF32"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SBRyd5zOeD60",
        "outputId": "e60c2207-078d-4b9b-9f2e-546e177a30c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 - 1s - loss: 0.2012 - accuracy: 1.0000 - 1s/epoch - 1s/step\n",
            "Test score: 0.2012208253145218\n",
            "Test accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "movie_reviews_train = [\n",
        "         {'review': 'this is the best movie', 'sentiment': 'positive'},\n",
        "         {'review': 'i recommend you watch this movie', 'sentiment': 'positive'},\n",
        "         {'review': 'it was waste of money and time', 'sentiment': 'negative'},\n",
        "         {'review': 'the worst movie ever', 'sentiment': 'negative'}\n",
        "    ]\n",
        "\"\"\"\n",
        "movie_reviews_test = [\n",
        "         {'review': 'it is better movie', 'sentiment': 'positive'},\n",
        "         {'review': 'i suggest you see this movie', 'sentiment': 'positive'},\n",
        "         {'review': 'it was just throwing 20 dollars away', 'sentiment': 'negative'},\n",
        "         {'review': 'worse than any show', 'sentiment': 'negative'},\n",
        "         {'review': 'nice movie, so love it', 'sentiment': 'positive'},\n",
        "         {'review': 'terrible', 'sentiment': 'negative'}\n",
        "    ]\n",
        "test_df = pd.DataFrame(movie_reviews_test)\n",
        "\n",
        "test_X, test_Y = preprocess(test_df)\n",
        "\n",
        "score, acc = model.evaluate(test_X, test_Y, verbose=2)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7nUTwkHeD60"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "10.Word2Vec_LSTM.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
